{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae04332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/h/Opengate')\n",
    "from config import scanner, phantom, reconstruction\n",
    "import numpy as np\n",
    "\n",
    "lor_data = np.load(\"/home/h/Opengate/Post_process/Outputs/coincidence_pairs.npz\")\n",
    "attenuation_map = np.load(\"/home/h/Opengate/Simulation/Outputs/mu_map.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aabb204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sensitivity map...\n",
      "  Scanner: inner_radius=235.4mm, axial_length=296.0mm\n",
      "  Image: 100³ voxels (2.0mm)\n",
      "  Range: [0.405293, 0.993993]\n",
      "  Non-zero voxels: 1,000,000\n"
     ]
    }
   ],
   "source": [
    "from numba import njit, prange\n",
    "import math\n",
    "\n",
    "@njit\n",
    "def compute_sensitivity_2d(r, z, inner_radius, z_bottom, z_top, axial_length):\n",
    "    if r >= inner_radius:\n",
    "        return 0.0\n",
    "    \n",
    "    d_to_wall = inner_radius - r\n",
    "    \n",
    "    z_min_near = z - d_to_wall * (z - z_bottom) / inner_radius\n",
    "    z_max_near = z + d_to_wall * (z_top - z) / inner_radius\n",
    "    z_min_near = max(z_bottom, z_min_near)\n",
    "    z_max_near = min(z_top, z_max_near)\n",
    "    \n",
    "    d_through = inner_radius + r\n",
    "    scale = d_through / (inner_radius + r) if (inner_radius + r) > 0 else 1\n",
    "    z_min_far = z - (z - z_bottom) * scale\n",
    "    z_max_far = z + (z_top - z) * scale\n",
    "    z_min_far = max(z_bottom, z_min_far)\n",
    "    z_max_far = min(z_top, z_max_far)\n",
    "    \n",
    "    range_near = max(0.0, z_max_near - z_min_near)\n",
    "    range_far = max(0.0, z_max_far - z_min_far)\n",
    "    \n",
    "    sensitivity = range_near * range_far / (axial_length ** 2)\n",
    "    return sensitivity\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def generate_sensitivity_volume(n_voxels, voxel_size, fov, inner_radius, z_bottom, z_top, axial_length):\n",
    "    half_fov = fov / 2\n",
    "    volume = np.zeros((n_voxels, n_voxels, n_voxels), dtype=np.float32)\n",
    "    \n",
    "    for i in prange(n_voxels):\n",
    "        x = -half_fov + (i + 0.5) * voxel_size\n",
    "        for j in range(n_voxels):\n",
    "            y = -half_fov + (j + 0.5) * voxel_size\n",
    "            r = math.sqrt(x**2 + y**2)\n",
    "            \n",
    "            for k in range(n_voxels):\n",
    "                z = -half_fov + (k + 0.5) * voxel_size\n",
    "                volume[i, j, k] = compute_sensitivity_2d(r, z, inner_radius, z_bottom, z_top, axial_length)\n",
    "    \n",
    "    return volume\n",
    "\n",
    "\n",
    "def create_sensitivity_map(voxel_size, fov):\n",
    "    \"\"\"\n",
    "    Wrapper to create sensitivity map with scanner geometry.\n",
    "    \"\"\"\n",
    "    n_voxels = int(fov / voxel_size)\n",
    "    \n",
    "    print(f\"Generating sensitivity map...\")\n",
    "    print(f\"  Scanner: inner_radius={scanner.inner_radius:.1f}mm, axial_length={scanner.axial_length:.1f}mm\")\n",
    "    print(f\"  Image: {n_voxels}³ voxels ({voxel_size}mm)\")\n",
    "    \n",
    "    sensitivity = generate_sensitivity_volume(\n",
    "        n_voxels, voxel_size, fov,\n",
    "        scanner.inner_radius, scanner.z_bottom,\n",
    "        scanner.z_top, scanner.axial_length\n",
    "    )\n",
    "    \n",
    "    # Avoid division by zero - set minimum sensitivity\n",
    "    sensitivity = np.maximum(sensitivity, 1e-10)\n",
    "    \n",
    "    print(f\"  Range: [{sensitivity.min():.6f}, {sensitivity.max():.6f}]\")\n",
    "    print(f\"  Non-zero voxels: {np.sum(sensitivity > 1e-10):,}\")\n",
    "    \n",
    "    return sensitivity\n",
    "\n",
    "\n",
    "sensitivity = create_sensitivity_map(voxel_size=reconstruction.voxel_size, fov=reconstruction.fov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0683f02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23720a3d792740cbab90de1eb67631c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(geometries=[], gradient_opacity=0.22, point_sets=[], rendered_image=<itk.itkImagePython.itkImageF3; pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itkwidgets import view\n",
    "\n",
    "# unfiltered_backprojection.shape\n",
    "view(sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf776bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIDDON PROJECTOR WITH ATTENUATION\n",
    "# =============================================================================\n",
    "@njit\n",
    "def siddon_single_lor_with_attenuation(p1, p2, image, mu_map, n_voxels, voxel_size, half_fov, forward=True):\n",
    "    \"\"\"\n",
    "    Siddon's algorithm with attenuation correction.\n",
    "    \n",
    "    For forward projection: returns attenuated line integral\n",
    "    For back projection: returns attenuation-weighted contribution to each voxel\n",
    "    \n",
    "    Args:\n",
    "        p1, p2: LOR endpoints\n",
    "        image: Activity image (for forward) or will be modified (for back)\n",
    "        mu_map: Attenuation map (μ in mm^-1)\n",
    "        forward: If True, compute forward projection. If False, backproject.\n",
    "    \n",
    "    Returns:\n",
    "        For forward: line integral value\n",
    "        For back: None (modifies image in place)\n",
    "    \"\"\"\n",
    "    # Convert to voxel coordinates\n",
    "    p1_v = np.empty(3)\n",
    "    p2_v = np.empty(3)\n",
    "    for i in range(3):\n",
    "        p1_v[i] = (p1[i] + half_fov) / voxel_size\n",
    "        p2_v[i] = (p2[i] + half_fov) / voxel_size\n",
    "    \n",
    "    # Direction vector\n",
    "    d = np.empty(3)\n",
    "    for i in range(3):\n",
    "        d[i] = p2_v[i] - p1_v[i]\n",
    "    \n",
    "    lor_length = np.sqrt(d[0]**2 + d[1]**2 + d[2]**2)\n",
    "    \n",
    "    if lor_length < 1e-10:\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute alpha bounds\n",
    "    alpha_min = 0.0\n",
    "    alpha_max = 1.0\n",
    "    \n",
    "    for i in range(3):\n",
    "        if abs(d[i]) > 1e-10:\n",
    "            a1 = (0 - p1_v[i]) / d[i]\n",
    "            a2 = (n_voxels - p1_v[i]) / d[i]\n",
    "            if a1 > a2:\n",
    "                a1, a2 = a2, a1\n",
    "            alpha_min = max(alpha_min, a1)\n",
    "            alpha_max = min(alpha_max, a2)\n",
    "    \n",
    "    if alpha_min >= alpha_max:\n",
    "        return 0.0\n",
    "    \n",
    "    # Collect alpha values at voxel boundaries\n",
    "    max_intersections = 3 * n_voxels + 3\n",
    "    alphas = np.empty(max_intersections, dtype=np.float64)\n",
    "    n_alphas = 0\n",
    "    \n",
    "    alphas[n_alphas] = alpha_min\n",
    "    n_alphas += 1\n",
    "    alphas[n_alphas] = alpha_max\n",
    "    n_alphas += 1\n",
    "    \n",
    "    for axis in range(3):\n",
    "        if abs(d[axis]) < 1e-10:\n",
    "            continue\n",
    "        \n",
    "        if d[axis] > 0:\n",
    "            i_min = int(np.ceil(p1_v[axis] + alpha_min * d[axis]))\n",
    "            i_max = int(np.floor(p1_v[axis] + alpha_max * d[axis]))\n",
    "        else:\n",
    "            i_min = int(np.ceil(p1_v[axis] + alpha_max * d[axis]))\n",
    "            i_max = int(np.floor(p1_v[axis] + alpha_min * d[axis]))\n",
    "        \n",
    "        i_min = max(0, min(n_voxels, i_min))\n",
    "        i_max = max(0, min(n_voxels, i_max))\n",
    "        \n",
    "        for i in range(i_min, i_max + 1):\n",
    "            alpha = (i - p1_v[axis]) / d[axis]\n",
    "            if alpha_min < alpha < alpha_max:\n",
    "                alphas[n_alphas] = alpha\n",
    "                n_alphas += 1\n",
    "    \n",
    "    # Sort alphas\n",
    "    alphas_sorted = np.sort(alphas[:n_alphas])\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_alphas = np.empty(n_alphas, dtype=np.float64)\n",
    "    n_unique = 0\n",
    "    prev = -1e10\n",
    "    for i in range(len(alphas_sorted)):\n",
    "        if alphas_sorted[i] - prev > 1e-10:\n",
    "            unique_alphas[n_unique] = alphas_sorted[i]\n",
    "            n_unique += 1\n",
    "            prev = alphas_sorted[i]\n",
    "    \n",
    "    if n_unique < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # First pass: compute total attenuation along LOR (for ACF)\n",
    "    total_mu_length = 0.0\n",
    "    for i in range(n_unique - 1):\n",
    "        alpha_mid = (unique_alphas[i] + unique_alphas[i + 1]) / 2.0\n",
    "        \n",
    "        ix = int(p1_v[0] + alpha_mid * d[0])\n",
    "        iy = int(p1_v[1] + alpha_mid * d[1])\n",
    "        iz = int(p1_v[2] + alpha_mid * d[2])\n",
    "        \n",
    "        if 0 <= ix < n_voxels and 0 <= iy < n_voxels and 0 <= iz < n_voxels:\n",
    "            segment_length = (unique_alphas[i + 1] - unique_alphas[i]) * lor_length * voxel_size\n",
    "            total_mu_length += mu_map[ix, iy, iz] * segment_length\n",
    "    \n",
    "    # Attenuation correction factor\n",
    "    acf = np.exp(total_mu_length) if total_mu_length > 0 else 1.0\n",
    "    \n",
    "    if forward:\n",
    "        # Forward projection: compute line integral with attenuation\n",
    "        line_integral = 0.0\n",
    "        \n",
    "        for i in range(n_unique - 1):\n",
    "            alpha_mid = (unique_alphas[i] + unique_alphas[i + 1]) / 2.0\n",
    "            \n",
    "            ix = int(p1_v[0] + alpha_mid * d[0])\n",
    "            iy = int(p1_v[1] + alpha_mid * d[1])\n",
    "            iz = int(p1_v[2] + alpha_mid * d[2])\n",
    "            \n",
    "            if 0 <= ix < n_voxels and 0 <= iy < n_voxels and 0 <= iz < n_voxels:\n",
    "                segment_length = (unique_alphas[i + 1] - unique_alphas[i]) * lor_length * voxel_size\n",
    "                line_integral += image[ix, iy, iz] * segment_length\n",
    "        \n",
    "        # Apply attenuation (photons are attenuated, so divide by ACF)\n",
    "        return line_integral / acf\n",
    "    \n",
    "    else:\n",
    "        # Backprojection: distribute value to voxels with attenuation weighting\n",
    "        return acf  # Return ACF for external handling\n",
    "\n",
    "\n",
    "@njit\n",
    "def forward_project_lor(p1, p2, image, mu_map, n_voxels, voxel_size, half_fov):\n",
    "    \"\"\"Forward project a single LOR with attenuation.\"\"\"\n",
    "    return siddon_single_lor_with_attenuation(p1, p2, image, mu_map, n_voxels, voxel_size, half_fov, forward=True)\n",
    "\n",
    "\n",
    "@njit\n",
    "def siddon_backproject_lor(p1, p2, n_voxels, voxel_size, half_fov, value, image):\n",
    "    \"\"\"\n",
    "    Backproject a single LOR value into image.\n",
    "    \"\"\"\n",
    "    # Convert to voxel coordinates\n",
    "    p1_v = np.empty(3)\n",
    "    p2_v = np.empty(3)\n",
    "    for i in range(3):\n",
    "        p1_v[i] = (p1[i] + half_fov) / voxel_size\n",
    "        p2_v[i] = (p2[i] + half_fov) / voxel_size\n",
    "    \n",
    "    d = np.empty(3)\n",
    "    for i in range(3):\n",
    "        d[i] = p2_v[i] - p1_v[i]\n",
    "    \n",
    "    lor_length = np.sqrt(d[0]**2 + d[1]**2 + d[2]**2)\n",
    "    \n",
    "    if lor_length < 1e-10:\n",
    "        return\n",
    "    \n",
    "    alpha_min = 0.0\n",
    "    alpha_max = 1.0\n",
    "    \n",
    "    for i in range(3):\n",
    "        if abs(d[i]) > 1e-10:\n",
    "            a1 = (0 - p1_v[i]) / d[i]\n",
    "            a2 = (n_voxels - p1_v[i]) / d[i]\n",
    "            if a1 > a2:\n",
    "                a1, a2 = a2, a1\n",
    "            alpha_min = max(alpha_min, a1)\n",
    "            alpha_max = min(alpha_max, a2)\n",
    "    \n",
    "    if alpha_min >= alpha_max:\n",
    "        return\n",
    "    \n",
    "    max_intersections = 3 * n_voxels + 3\n",
    "    alphas = np.empty(max_intersections, dtype=np.float64)\n",
    "    n_alphas = 0\n",
    "    \n",
    "    alphas[n_alphas] = alpha_min\n",
    "    n_alphas += 1\n",
    "    alphas[n_alphas] = alpha_max\n",
    "    n_alphas += 1\n",
    "    \n",
    "    for axis in range(3):\n",
    "        if abs(d[axis]) < 1e-10:\n",
    "            continue\n",
    "        \n",
    "        if d[axis] > 0:\n",
    "            i_min = int(np.ceil(p1_v[axis] + alpha_min * d[axis]))\n",
    "            i_max = int(np.floor(p1_v[axis] + alpha_max * d[axis]))\n",
    "        else:\n",
    "            i_min = int(np.ceil(p1_v[axis] + alpha_max * d[axis]))\n",
    "            i_max = int(np.floor(p1_v[axis] + alpha_min * d[axis]))\n",
    "        \n",
    "        i_min = max(0, min(n_voxels, i_min))\n",
    "        i_max = max(0, min(n_voxels, i_max))\n",
    "        \n",
    "        for i in range(i_min, i_max + 1):\n",
    "            alpha = (i - p1_v[axis]) / d[axis]\n",
    "            if alpha_min < alpha < alpha_max:\n",
    "                alphas[n_alphas] = alpha\n",
    "                n_alphas += 1\n",
    "    \n",
    "    alphas_sorted = np.sort(alphas[:n_alphas])\n",
    "    \n",
    "    unique_alphas = np.empty(n_alphas, dtype=np.float64)\n",
    "    n_unique = 0\n",
    "    prev = -1e10\n",
    "    for i in range(len(alphas_sorted)):\n",
    "        if alphas_sorted[i] - prev > 1e-10:\n",
    "            unique_alphas[n_unique] = alphas_sorted[i]\n",
    "            n_unique += 1\n",
    "            prev = alphas_sorted[i]\n",
    "    \n",
    "    if n_unique < 2:\n",
    "        return\n",
    "    \n",
    "    for i in range(n_unique - 1):\n",
    "        alpha_mid = (unique_alphas[i] + unique_alphas[i + 1]) / 2.0\n",
    "        \n",
    "        ix = int(p1_v[0] + alpha_mid * d[0])\n",
    "        iy = int(p1_v[1] + alpha_mid * d[1])\n",
    "        iz = int(p1_v[2] + alpha_mid * d[2])\n",
    "        \n",
    "        if 0 <= ix < n_voxels and 0 <= iy < n_voxels and 0 <= iz < n_voxels:\n",
    "            segment_length = (unique_alphas[i + 1] - unique_alphas[i]) * lor_length * voxel_size\n",
    "            image[ix, iy, iz] += value * segment_length\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# OSEM RECONSTRUCTION\n",
    "# =============================================================================\n",
    "def osem_reconstruct(xyz1, xyz2, mu_map=None, voxel_size=1.0, fov=200.0, \n",
    "                     n_iterations=2, n_subsets=8, verbose=True):\n",
    "    \"\"\"\n",
    "    OSEM reconstruction with attenuation correction.\n",
    "    \n",
    "    Args:\n",
    "        xyz1, xyz2: LOR endpoints (N x 3 arrays)\n",
    "        mu_map: Attenuation map array (if None, generates analytically)\n",
    "        voxel_size: Voxel size in mm\n",
    "        fov: Field of view in mm\n",
    "        n_iterations: Number of full iterations\n",
    "        n_subsets: Number of subsets per iteration\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        Reconstructed image (3D array)\n",
    "    \"\"\"\n",
    "    n_voxels = int(fov / voxel_size)\n",
    "    half_fov = fov / 2\n",
    "    n_lors = len(xyz1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"OSEM Reconstruction\")\n",
    "        print(f\"  LORs: {n_lors:,}\")\n",
    "        print(f\"  Image: {n_voxels}³ voxels ({voxel_size}mm)\")\n",
    "        print(f\"  Iterations: {n_iterations}, Subsets: {n_subsets}\")\n",
    "    \n",
    "    # Load or generate attenuation map\n",
    "    if verbose:\n",
    "        print(\"\\nAttenuation map...\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Using provided μ-map, shape: {mu_map.shape}\")\n",
    "    \n",
    "    if mu_map.shape != (n_voxels, n_voxels, n_voxels):\n",
    "        raise ValueError(\n",
    "            f\"μ-map shape {mu_map.shape} doesn't match expected \"\n",
    "            f\"({n_voxels}, {n_voxels}, {n_voxels}). \"\n",
    "            f\"Check voxel_size and fov parameters.\"\n",
    "        )\n",
    "    \n",
    "    mu_map = mu_map.astype(np.float32)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"  Non-zero voxels: {np.sum(mu_map > 0):,}\")\n",
    "        print(f\"  μ range: [{mu_map.min():.6f}, {mu_map.max():.6f}] mm⁻¹\")\n",
    "    # Initialise image (uniform positive value)\n",
    "    image = np.ones((n_voxels, n_voxels, n_voxels), dtype=np.float32)\n",
    "    \n",
    "    # Create subset indices\n",
    "    subset_indices = [np.arange(i, n_lors, n_subsets) for i in range(n_subsets)]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStarting iterations...\")\n",
    "    \n",
    "    # OSEM iterations\n",
    "    for iteration in range(n_iterations):\n",
    "        for subset_idx in range(n_subsets):\n",
    "            indices = subset_indices[subset_idx]\n",
    "            n_subset_lors = len(indices)\n",
    "            \n",
    "            # Forward project current estimate\n",
    "            forward_proj = np.zeros(n_subset_lors, dtype=np.float32)\n",
    "            \n",
    "            for i, lor_idx in enumerate(indices):\n",
    "                p1 = xyz1[lor_idx].astype(np.float64)\n",
    "                p2 = xyz2[lor_idx].astype(np.float64)\n",
    "                forward_proj[i] = forward_project_lor(\n",
    "                    p1, p2, image, mu_map, n_voxels, voxel_size, half_fov\n",
    "                )\n",
    "            \n",
    "            # Compute ratio (measured / estimated)\n",
    "            # Measured = 1 for each detected LOR\n",
    "            ratios = np.ones(n_subset_lors, dtype=np.float32)\n",
    "            mask = forward_proj > 1e-10\n",
    "            ratios[mask] = 1.0 / forward_proj[mask]\n",
    "            \n",
    "            # Backproject ratios\n",
    "            backproj = np.zeros((n_voxels, n_voxels, n_voxels), dtype=np.float32)\n",
    "            \n",
    "            for i, lor_idx in enumerate(indices):\n",
    "                p1 = xyz1[lor_idx].astype(np.float64)\n",
    "                p2 = xyz2[lor_idx].astype(np.float64)\n",
    "                siddon_backproject_lor(\n",
    "                    p1, p2, n_voxels, voxel_size, half_fov, ratios[i], backproj\n",
    "                )\n",
    "            \n",
    "            # Update image: image *= backproj / sensitivity\n",
    "            # Scale by number of subsets to account for subset sampling\n",
    "            update = backproj / sensitivity\n",
    "            image *= update\n",
    "            \n",
    "            # Enforce positivity\n",
    "            image = np.maximum(image, 1e-10)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Iteration {iteration+1}/{n_iterations}, \"\n",
    "                      f\"Subset {subset_idx+1}/{n_subsets}, \"\n",
    "                      f\"Image range: [{image.min():.4f}, {image.max():.4f}]\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nReconstruction complete!\")\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RUN RECONSTRUCTION\n",
    "# =============================================================================\n",
    "# Assuming 'data' contains xyz1 and xyz2 from your LOR extraction\n",
    "osem_image = osem_reconstruct(\n",
    "    lor_data['xyz1'], lor_data['xyz2'],\n",
    "    mu_map=attenuation_map,\n",
    "    voxel_size=reconstruction.voxel_size,\n",
    "    fov=reconstruction.fov,\n",
    "    n_iterations=reconstruction.n_iterations,\n",
    "    n_subsets=reconstruction.n_subsets,\n",
    ")\n",
    "\n",
    "# # Save result\n",
    "# np.save(\"osem_reconstruction.npy\", osem_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb8e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view(osem_image)\n",
    "# osem_image.mean()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opengate_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
