{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae04332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/h/Opengate')\n",
    "from config import scanner, phantom, reconstruction\n",
    "import numpy as np\n",
    "\n",
    "lor_data = np.load(\"/home/h/Opengate/Post_process/Outputs/coincidence_pairs.npz\")\n",
    "attenuation_map = np.load(\"/home/h/Opengate/Simulation/Outputs/mu_map.npy\")\n",
    "ground_truth_activity = np.load(\"/home/h/Opengate/Simulation/Outputs/activity_map.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103bcc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analytical Sensitivity Map for Cylindrical PET Scanner\n",
    "=======================================================\n",
    "Implements ray-casting geometry from 2D cross-section, revolved for 3D.\n",
    "\n",
    "For each point at (r, z):\n",
    "- Near wall at distance (inner_radius - r)\n",
    "- Far wall at distance (inner_radius + r)  \n",
    "- Cast rays from far wall edges through point -> valid region on near wall\n",
    "- Cast rays from near wall edges through point -> valid region on far wall\n",
    "- Sensitivity = sum of valid detector lengths\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "import math\n",
    "\n",
    "\n",
    "@njit\n",
    "def compute_valid_length_on_wall(\n",
    "    point_r: float,\n",
    "    point_z: float,\n",
    "    target_wall_dist: float,\n",
    "    source_wall_dist: float,\n",
    "    z_bottom: float,\n",
    "    z_top: float\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute valid detector length on target wall, limited by source wall edges.\n",
    "    \n",
    "    Uses similar triangles: a ray from source wall edge through the point\n",
    "    projects onto the target wall at a z-offset scaled by the distance ratio.\n",
    "    \n",
    "    Args:\n",
    "        point_r: Radial distance of point from axis (not used directly, \n",
    "                 but wall distances are derived from it)\n",
    "        point_z: Axial position of point\n",
    "        target_wall_dist: Distance from point to target wall\n",
    "        source_wall_dist: Distance from point to source wall (opposite wall)\n",
    "        z_bottom: Bottom of detector (axial)\n",
    "        z_top: Top of detector (axial)\n",
    "    \n",
    "    Returns:\n",
    "        Valid detector length on target wall (mm)\n",
    "    \"\"\"\n",
    "    # Ray from source wall top edge through point -> where on target wall?\n",
    "    # Similar triangles: z_offset scales with distance ratio\n",
    "    # From source top (z_top) through point (point_z) to target wall\n",
    "    # The ray continues past the point by ratio (target_wall_dist / source_wall_dist)\n",
    "    \n",
    "    total_dist = target_wall_dist + source_wall_dist\n",
    "    \n",
    "    if total_dist < 1e-10:\n",
    "        return 0.0\n",
    "    \n",
    "    # Ray from source_top through point to target wall\n",
    "    # Point is at fraction (source_wall_dist / total_dist) along the ray\n",
    "    # z at target = point_z + (point_z - z_top) * (target_wall_dist / source_wall_dist)\n",
    "    # Simplifies to: z_target = point_z - (z_top - point_z) * (target_wall_dist / source_wall_dist)\n",
    "    \n",
    "    ratio = target_wall_dist / source_wall_dist\n",
    "    \n",
    "    z_from_source_top = point_z - (z_top - point_z) * ratio\n",
    "    z_from_source_bottom = point_z - (z_bottom - point_z) * ratio\n",
    "    \n",
    "    # The \"shadow\" region on target wall\n",
    "    shadow_min = min(z_from_source_top, z_from_source_bottom)\n",
    "    shadow_max = max(z_from_source_top, z_from_source_bottom)\n",
    "    \n",
    "    # Intersect with physical wall extent\n",
    "    valid_min = max(z_bottom, shadow_min)\n",
    "    valid_max = min(z_top, shadow_max)\n",
    "    \n",
    "    return max(0.0, valid_max - valid_min)\n",
    "\n",
    "\n",
    "@njit\n",
    "def compute_sensitivity_at_point(\n",
    "    r: float,\n",
    "    z: float,\n",
    "    inner_radius: float,\n",
    "    z_bottom: float,\n",
    "    z_top: float\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute sensitivity for a point at (r, z) in cylindrical coordinates.\n",
    "    \n",
    "    Sensitivity = valid_length_near_wall + valid_length_far_wall\n",
    "    \n",
    "    Args:\n",
    "        r: Radial distance from central axis\n",
    "        z: Axial position\n",
    "        inner_radius: Scanner bore radius\n",
    "        z_bottom: Bottom of detector\n",
    "        z_top: Top of detector\n",
    "    \n",
    "    Returns:\n",
    "        Sensitivity (total valid detector length in mm)\n",
    "    \"\"\"\n",
    "    # Point must be inside bore\n",
    "    if r >= inner_radius:\n",
    "        return 0.0\n",
    "    \n",
    "    # Distance to near wall (closer side of cylinder)\n",
    "    near_wall_dist = inner_radius - r\n",
    "    # Distance to far wall (opposite side, through axis)\n",
    "    far_wall_dist = inner_radius + r\n",
    "    \n",
    "    # Valid length on near wall (limited by far wall edges)\n",
    "    near_length = compute_valid_length_on_wall(\n",
    "        r, z, near_wall_dist, far_wall_dist, z_bottom, z_top\n",
    "    )\n",
    "    \n",
    "    # Valid length on far wall (limited by near wall edges)\n",
    "    far_length = compute_valid_length_on_wall(\n",
    "        r, z, far_wall_dist, near_wall_dist, z_bottom, z_top\n",
    "    )\n",
    "    return near_length * far_length\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def generate_sensitivity_volume(\n",
    "    n_voxels: int,\n",
    "    voxel_size: float,\n",
    "    half_fov: float,\n",
    "    inner_radius: float,\n",
    "    z_bottom: float,\n",
    "    z_top: float\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate 3D sensitivity volume by revolving 2D sensitivity map.\n",
    "    \n",
    "    Args:\n",
    "        n_voxels: Number of voxels per dimension\n",
    "        voxel_size: Size of each voxel (mm)\n",
    "        half_fov: Half of field of view (mm)\n",
    "        inner_radius: Scanner bore radius (mm)\n",
    "        z_bottom: Bottom of detector (mm)\n",
    "        z_top: Top of detector (mm)\n",
    "    \n",
    "    Returns:\n",
    "        3D sensitivity volume (n_voxels, n_voxels, n_voxels)\n",
    "    \"\"\"\n",
    "    volume = np.zeros((n_voxels, n_voxels, n_voxels), dtype=np.float32)\n",
    "    \n",
    "    for i in prange(n_voxels):\n",
    "        x = -half_fov + (i + 0.5) * voxel_size\n",
    "        for j in range(n_voxels):\n",
    "            y = -half_fov + (j + 0.5) * voxel_size\n",
    "            r = math.sqrt(x * x + y * y)\n",
    "            \n",
    "            # Skip points outside bore\n",
    "            if r >= inner_radius:\n",
    "                continue\n",
    "                \n",
    "            for k in range(n_voxels):\n",
    "                z = -half_fov + (k + 0.5) * voxel_size\n",
    "                volume[i, j, k] = compute_sensitivity_at_point(\n",
    "                    r, z, inner_radius, z_bottom, z_top\n",
    "                )\n",
    "    \n",
    "    return volume\n",
    "\n",
    "\n",
    "def create_sensitivity_map(scanner, reconstruction):\n",
    "    \"\"\"\n",
    "    Create sensitivity map from scanner and reconstruction config.\n",
    "    \n",
    "    Args:\n",
    "        scanner: Scanner config object with inner_radius, z_bottom, z_top\n",
    "        reconstruction: Reconstruction config with voxel_size, fov, n_voxels\n",
    "    \n",
    "    Returns:\n",
    "        3D numpy array of sensitivity values\n",
    "    \"\"\"\n",
    "    n_voxels = reconstruction.n_voxels\n",
    "    voxel_size = reconstruction.voxel_size\n",
    "    half_fov = reconstruction.half_fov\n",
    "    \n",
    "    print(f\"Generating sensitivity map...\")\n",
    "    print(f\"  Scanner: inner_radius={scanner.inner_radius:.1f}mm, \"\n",
    "          f\"axial_length={scanner.axial_length:.1f}mm\")\n",
    "    print(f\"  Image: {n_voxels}³ voxels ({voxel_size}mm isotropic)\")\n",
    "    \n",
    "    sensitivity = generate_sensitivity_volume(\n",
    "        n_voxels=n_voxels,\n",
    "        voxel_size=voxel_size,\n",
    "        half_fov=half_fov,\n",
    "        inner_radius=scanner.inner_radius,\n",
    "        z_bottom=scanner.z_bottom,\n",
    "        z_top=scanner.z_top\n",
    "    )\n",
    "    \n",
    "    # Avoid division by zero in reconstruction\n",
    "    sensitivity = np.maximum(sensitivity, 1e-10)\n",
    "    \n",
    "    print(f\"  Sensitivity range: [{sensitivity.min():.2f}, {sensitivity.max():.2f}] mm\")\n",
    "    print(f\"  Non-zero voxels: {np.sum(sensitivity > 1e-10):,}\")\n",
    "    \n",
    "    return sensitivity\n",
    "\n",
    "\n",
    "sensitivity = create_sensitivity_map(scanner, reconstruction)\n",
    "\n",
    "# Quick validation\n",
    "n = reconstruction.n_voxels\n",
    "centre = n // 2\n",
    "\n",
    "print(f\"\\nValidation:\")\n",
    "print(f\"  Centre sensitivity: {sensitivity[centre, centre, centre]:.2f} mm\")\n",
    "print(f\"  Max sensitivity: {sensitivity.max():.2f} mm\")\n",
    "\n",
    "# Check symmetry\n",
    "lr_diff = np.abs(sensitivity - np.flip(sensitivity, axis=0)).max()\n",
    "tb_diff = np.abs(sensitivity - np.flip(sensitivity, axis=1)).max()\n",
    "ax_diff = np.abs(sensitivity - np.flip(sensitivity, axis=2)).max()\n",
    "print(f\"  Symmetry errors (should be ~0): LR={lr_diff:.6f}, TB={tb_diff:.6f}, Ax={ax_diff:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "import math\n",
    "\n",
    "@njit\n",
    "def compute_sensitivity_2d(r, z, inner_radius, z_bottom, z_top, axial_length):\n",
    "    if r >= inner_radius:\n",
    "        return 0.0\n",
    "    \n",
    "    d_to_wall = inner_radius - r\n",
    "    \n",
    "    z_min_near = z - d_to_wall * (z - z_bottom) / inner_radius\n",
    "    z_max_near = z + d_to_wall * (z_top - z) / inner_radius\n",
    "    z_min_near = max(z_bottom, z_min_near)\n",
    "    z_max_near = min(z_top, z_max_near)\n",
    "    \n",
    "    d_through = inner_radius + r\n",
    "    scale = d_through / (inner_radius + r) if (inner_radius + r) > 0 else 1\n",
    "    z_min_far = z - (z - z_bottom) * scale\n",
    "    z_max_far = z + (z_top - z) * scale\n",
    "    z_min_far = max(z_bottom, z_min_far)\n",
    "    z_max_far = min(z_top, z_max_far)\n",
    "    \n",
    "    range_near = max(0.0, z_max_near - z_min_near)\n",
    "    range_far = max(0.0, z_max_far - z_min_far)\n",
    "    \n",
    "    sensitivity = range_near * range_far / (axial_length ** 2)\n",
    "    return sensitivity\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def generate_sensitivity_volume(n_voxels, voxel_size, fov, inner_radius, z_bottom, z_top, axial_length):\n",
    "    half_fov = fov / 2\n",
    "    volume = np.zeros((n_voxels, n_voxels, n_voxels), dtype=np.float32)\n",
    "    \n",
    "    for i in prange(n_voxels):\n",
    "        x = -half_fov + (i + 0.5) * voxel_size\n",
    "        for j in range(n_voxels):\n",
    "            y = -half_fov + (j + 0.5) * voxel_size\n",
    "            r = math.sqrt(x**2 + y**2)\n",
    "            \n",
    "            for k in range(n_voxels):\n",
    "                z = -half_fov + (k + 0.5) * voxel_size\n",
    "                volume[i, j, k] = compute_sensitivity_2d(r, z, inner_radius, z_bottom, z_top, axial_length)\n",
    "    \n",
    "    return volume\n",
    "\n",
    "\n",
    "def create_sensitivity_map(voxel_size, fov):\n",
    "    \"\"\"\n",
    "    Wrapper to create sensitivity map with scanner geometry.\n",
    "    \"\"\"\n",
    "    n_voxels = int(fov / voxel_size)\n",
    "    \n",
    "    print(f\"Generating sensitivity map...\")\n",
    "    print(f\"  Scanner: inner_radius={scanner.inner_radius:.1f}mm, axial_length={scanner.axial_length:.1f}mm\")\n",
    "    print(f\"  Image: {n_voxels}³ voxels ({voxel_size}mm)\")\n",
    "    \n",
    "    sensitivity = generate_sensitivity_volume(\n",
    "        n_voxels, voxel_size, fov,\n",
    "        scanner.inner_radius, scanner.z_bottom,\n",
    "        scanner.z_top, scanner.axial_length\n",
    "    )\n",
    "    \n",
    "    # Avoid division by zero - set minimum sensitivity\n",
    "    sensitivity = np.maximum(sensitivity, 1e-10)\n",
    "    \n",
    "    print(f\"  Range: [{sensitivity.min():.6f}, {sensitivity.max():.6f}]\")\n",
    "    print(f\"  Non-zero voxels: {np.sum(sensitivity > 1e-10):,}\")\n",
    "    \n",
    "    return sensitivity\n",
    "\n",
    "\n",
    "sensitivity = create_sensitivity_map(voxel_size=reconstruction.voxel_size, fov=reconstruction.fov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0683f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# sensitivity = sensitivity * 0 + 0.0001\n",
    "# Animate through slices\n",
    "fig = px.imshow(\n",
    "    sensitivity,\n",
    "    animation_frame=2,\n",
    "    color_continuous_scale='Hot',\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf776bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIDDON PROJECTOR WITH ATTENUATION\n",
    "# =============================================================================\n",
    "@njit\n",
    "def siddon_single_lor_with_attenuation(p1, p2, image, mu_map, n_voxels, voxel_size, half_fov, forward=True):\n",
    "    \"\"\"\n",
    "    Siddon's algorithm with attenuation correction.\n",
    "    \n",
    "    For forward projection: returns attenuated line integral\n",
    "    For back projection: returns attenuation-weighted contribution to each voxel\n",
    "    \n",
    "    Args:\n",
    "        p1, p2: LOR endpoints\n",
    "        image: Activity image (for forward) or will be modified (for back)\n",
    "        mu_map: Attenuation map (μ in mm^-1)\n",
    "        forward: If True, compute forward projection. If False, backproject.\n",
    "    \n",
    "    Returns:\n",
    "        For forward: line integral value\n",
    "        For back: None (modifies image in place)\n",
    "    \"\"\"\n",
    "    # Convert to voxel coordinates\n",
    "    p1_v = np.empty(3)\n",
    "    p2_v = np.empty(3)\n",
    "    for i in range(3):\n",
    "        p1_v[i] = (p1[i] + half_fov) / voxel_size\n",
    "        p2_v[i] = (p2[i] + half_fov) / voxel_size\n",
    "    \n",
    "    # Direction vector\n",
    "    d = np.empty(3)\n",
    "    for i in range(3):\n",
    "        d[i] = p2_v[i] - p1_v[i]\n",
    "    \n",
    "    lor_length = np.sqrt(d[0]**2 + d[1]**2 + d[2]**2)\n",
    "    \n",
    "    if lor_length < 1e-10:\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute alpha bounds\n",
    "    alpha_min = 0.0\n",
    "    alpha_max = 1.0\n",
    "    \n",
    "    for i in range(3):\n",
    "        if abs(d[i]) > 1e-10:\n",
    "            a1 = (0 - p1_v[i]) / d[i]\n",
    "            a2 = (n_voxels - p1_v[i]) / d[i]\n",
    "            if a1 > a2:\n",
    "                a1, a2 = a2, a1\n",
    "            alpha_min = max(alpha_min, a1)\n",
    "            alpha_max = min(alpha_max, a2)\n",
    "    \n",
    "    if alpha_min >= alpha_max:\n",
    "        return 0.0\n",
    "    \n",
    "    # Collect alpha values at voxel boundaries\n",
    "    max_intersections = 3 * n_voxels + 3\n",
    "    alphas = np.empty(max_intersections, dtype=np.float64)\n",
    "    n_alphas = 0\n",
    "    \n",
    "    alphas[n_alphas] = alpha_min\n",
    "    n_alphas += 1\n",
    "    alphas[n_alphas] = alpha_max\n",
    "    n_alphas += 1\n",
    "    \n",
    "    for axis in range(3):\n",
    "        if abs(d[axis]) < 1e-10:\n",
    "            continue\n",
    "        \n",
    "        if d[axis] > 0:\n",
    "            i_min = int(np.ceil(p1_v[axis] + alpha_min * d[axis]))\n",
    "            i_max = int(np.floor(p1_v[axis] + alpha_max * d[axis]))\n",
    "        else:\n",
    "            i_min = int(np.ceil(p1_v[axis] + alpha_max * d[axis]))\n",
    "            i_max = int(np.floor(p1_v[axis] + alpha_min * d[axis]))\n",
    "        \n",
    "        i_min = max(0, min(n_voxels, i_min))\n",
    "        i_max = max(0, min(n_voxels, i_max))\n",
    "        \n",
    "        for i in range(i_min, i_max + 1):\n",
    "            alpha = (i - p1_v[axis]) / d[axis]\n",
    "            if alpha_min < alpha < alpha_max:\n",
    "                alphas[n_alphas] = alpha\n",
    "                n_alphas += 1\n",
    "    \n",
    "    # Sort alphas\n",
    "    alphas_sorted = np.sort(alphas[:n_alphas])\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_alphas = np.empty(n_alphas, dtype=np.float64)\n",
    "    n_unique = 0\n",
    "    prev = -1e10\n",
    "    for i in range(len(alphas_sorted)):\n",
    "        if alphas_sorted[i] - prev > 1e-10:\n",
    "            unique_alphas[n_unique] = alphas_sorted[i]\n",
    "            n_unique += 1\n",
    "            prev = alphas_sorted[i]\n",
    "    \n",
    "    if n_unique < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # First pass: compute total attenuation along LOR (for ACF)\n",
    "    total_mu_length = 0.0\n",
    "    for i in range(n_unique - 1):\n",
    "        alpha_mid = (unique_alphas[i] + unique_alphas[i + 1]) / 2.0\n",
    "        \n",
    "        ix = int(p1_v[0] + alpha_mid * d[0])\n",
    "        iy = int(p1_v[1] + alpha_mid * d[1])\n",
    "        iz = int(p1_v[2] + alpha_mid * d[2])\n",
    "        \n",
    "        if 0 <= ix < n_voxels and 0 <= iy < n_voxels and 0 <= iz < n_voxels:\n",
    "            segment_length = (unique_alphas[i + 1] - unique_alphas[i]) * lor_length * voxel_size\n",
    "            total_mu_length += mu_map[ix, iy, iz] * segment_length\n",
    "    \n",
    "    # Attenuation correction factor\n",
    "    acf = np.exp(total_mu_length) if total_mu_length > 0 else 1.0\n",
    "    \n",
    "    if forward:\n",
    "        # Forward projection: compute line integral with attenuation\n",
    "        line_integral = 0.0\n",
    "        \n",
    "        for i in range(n_unique - 1):\n",
    "            alpha_mid = (unique_alphas[i] + unique_alphas[i + 1]) / 2.0\n",
    "            \n",
    "            ix = int(p1_v[0] + alpha_mid * d[0])\n",
    "            iy = int(p1_v[1] + alpha_mid * d[1])\n",
    "            iz = int(p1_v[2] + alpha_mid * d[2])\n",
    "            \n",
    "            if 0 <= ix < n_voxels and 0 <= iy < n_voxels and 0 <= iz < n_voxels:\n",
    "                segment_length = (unique_alphas[i + 1] - unique_alphas[i]) * lor_length * voxel_size\n",
    "                line_integral += image[ix, iy, iz] * segment_length\n",
    "        \n",
    "        # Apply attenuation (photons are attenuated, so divide by ACF)\n",
    "        return line_integral / acf\n",
    "    \n",
    "    else:\n",
    "        # Backprojection: distribute value to voxels with attenuation weighting\n",
    "        return acf  # Return ACF for external handling\n",
    "\n",
    "\n",
    "@njit\n",
    "def forward_project_lor(p1, p2, image, mu_map, n_voxels, voxel_size, half_fov):\n",
    "    \"\"\"Forward project a single LOR with attenuation.\"\"\"\n",
    "    return siddon_single_lor_with_attenuation(p1, p2, image, mu_map, n_voxels, voxel_size, half_fov, forward=True)\n",
    "\n",
    "\n",
    "@njit\n",
    "def siddon_backproject_lor(p1, p2, n_voxels, voxel_size, half_fov, value, image):\n",
    "    \"\"\"\n",
    "    Backproject a single LOR value into image.\n",
    "    \"\"\"\n",
    "    # Convert to voxel coordinates\n",
    "    p1_v = np.empty(3)\n",
    "    p2_v = np.empty(3)\n",
    "    for i in range(3):\n",
    "        p1_v[i] = (p1[i] + half_fov) / voxel_size\n",
    "        p2_v[i] = (p2[i] + half_fov) / voxel_size\n",
    "    \n",
    "    d = np.empty(3)\n",
    "    for i in range(3):\n",
    "        d[i] = p2_v[i] - p1_v[i]\n",
    "    \n",
    "    lor_length = np.sqrt(d[0]**2 + d[1]**2 + d[2]**2)\n",
    "    \n",
    "    if lor_length < 1e-10:\n",
    "        return\n",
    "    \n",
    "    alpha_min = 0.0\n",
    "    alpha_max = 1.0\n",
    "    \n",
    "    for i in range(3):\n",
    "        if abs(d[i]) > 1e-10:\n",
    "            a1 = (0 - p1_v[i]) / d[i]\n",
    "            a2 = (n_voxels - p1_v[i]) / d[i]\n",
    "            if a1 > a2:\n",
    "                a1, a2 = a2, a1\n",
    "            alpha_min = max(alpha_min, a1)\n",
    "            alpha_max = min(alpha_max, a2)\n",
    "    \n",
    "    if alpha_min >= alpha_max:\n",
    "        return\n",
    "    \n",
    "    max_intersections = 3 * n_voxels + 3\n",
    "    alphas = np.empty(max_intersections, dtype=np.float64)\n",
    "    n_alphas = 0\n",
    "    \n",
    "    alphas[n_alphas] = alpha_min\n",
    "    n_alphas += 1\n",
    "    alphas[n_alphas] = alpha_max\n",
    "    n_alphas += 1\n",
    "    \n",
    "    for axis in range(3):\n",
    "        if abs(d[axis]) < 1e-10:\n",
    "            continue\n",
    "        \n",
    "        if d[axis] > 0:\n",
    "            i_min = int(np.ceil(p1_v[axis] + alpha_min * d[axis]))\n",
    "            i_max = int(np.floor(p1_v[axis] + alpha_max * d[axis]))\n",
    "        else:\n",
    "            i_min = int(np.ceil(p1_v[axis] + alpha_max * d[axis]))\n",
    "            i_max = int(np.floor(p1_v[axis] + alpha_min * d[axis]))\n",
    "        \n",
    "        i_min = max(0, min(n_voxels, i_min))\n",
    "        i_max = max(0, min(n_voxels, i_max))\n",
    "        \n",
    "        for i in range(i_min, i_max + 1):\n",
    "            alpha = (i - p1_v[axis]) / d[axis]\n",
    "            if alpha_min < alpha < alpha_max:\n",
    "                alphas[n_alphas] = alpha\n",
    "                n_alphas += 1\n",
    "    \n",
    "    alphas_sorted = np.sort(alphas[:n_alphas])\n",
    "    \n",
    "    unique_alphas = np.empty(n_alphas, dtype=np.float64)\n",
    "    n_unique = 0\n",
    "    prev = -1e10\n",
    "    for i in range(len(alphas_sorted)):\n",
    "        if alphas_sorted[i] - prev > 1e-10:\n",
    "            unique_alphas[n_unique] = alphas_sorted[i]\n",
    "            n_unique += 1\n",
    "            prev = alphas_sorted[i]\n",
    "    \n",
    "    if n_unique < 2:\n",
    "        return\n",
    "    \n",
    "    for i in range(n_unique - 1):\n",
    "        alpha_mid = (unique_alphas[i] + unique_alphas[i + 1]) / 2.0\n",
    "        \n",
    "        ix = int(p1_v[0] + alpha_mid * d[0])\n",
    "        iy = int(p1_v[1] + alpha_mid * d[1])\n",
    "        iz = int(p1_v[2] + alpha_mid * d[2])\n",
    "        \n",
    "        if 0 <= ix < n_voxels and 0 <= iy < n_voxels and 0 <= iz < n_voxels:\n",
    "            segment_length = (unique_alphas[i + 1] - unique_alphas[i]) * lor_length * voxel_size\n",
    "            image[ix, iy, iz] += value * segment_length\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# OSEM RECONSTRUCTION\n",
    "# =============================================================================\n",
    "def osem_reconstruct(xyz1, xyz2, mu_map=None, voxel_size=1.0, fov=200.0, \n",
    "                     n_iterations=2, n_subsets=8, verbose=True):\n",
    "    \"\"\"\n",
    "    OSEM reconstruction with attenuation correction.\n",
    "    \n",
    "    Args:\n",
    "        xyz1, xyz2: LOR endpoints (N x 3 arrays)\n",
    "        mu_map: Attenuation map array (if None, generates analytically)\n",
    "        voxel_size: Voxel size in mm\n",
    "        fov: Field of view in mm\n",
    "        n_iterations: Number of full iterations\n",
    "        n_subsets: Number of subsets per iteration\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        Reconstructed image (3D array)\n",
    "    \"\"\"\n",
    "    n_voxels = int(fov / voxel_size)\n",
    "    half_fov = fov / 2\n",
    "    n_lors = len(xyz1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"OSEM Reconstruction\")\n",
    "        print(f\"  LORs: {n_lors:,}\")\n",
    "        print(f\"  Image: {n_voxels}³ voxels ({voxel_size}mm)\")\n",
    "        print(f\"  Iterations: {n_iterations}, Subsets: {n_subsets}\")\n",
    "    \n",
    "    # Load or generate attenuation map\n",
    "    if verbose:\n",
    "        print(\"\\nAttenuation map...\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Using provided μ-map, shape: {mu_map.shape}\")\n",
    "    \n",
    "    if mu_map.shape != (n_voxels, n_voxels, n_voxels):\n",
    "        raise ValueError(\n",
    "            f\"μ-map shape {mu_map.shape} doesn't match expected \"\n",
    "            f\"({n_voxels}, {n_voxels}, {n_voxels}). \"\n",
    "            f\"Check voxel_size and fov parameters.\"\n",
    "        )\n",
    "    \n",
    "    mu_map = mu_map.astype(np.float32)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"  Non-zero voxels: {np.sum(mu_map > 0):,}\")\n",
    "        print(f\"  μ range: [{mu_map.min():.6f}, {mu_map.max():.6f}] mm⁻¹\")\n",
    "    # Initialise image (uniform positive value)\n",
    "    image = np.ones((n_voxels, n_voxels, n_voxels), dtype=np.float32)\n",
    "    \n",
    "    # Create subset indices\n",
    "    subset_indices = [np.arange(i, n_lors, n_subsets) for i in range(n_subsets)]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStarting iterations...\")\n",
    "    \n",
    "    # OSEM iterations\n",
    "    for iteration in range(n_iterations):\n",
    "        for subset_idx in range(n_subsets):\n",
    "            indices = subset_indices[subset_idx]\n",
    "            n_subset_lors = len(indices)\n",
    "            \n",
    "            # Forward project current estimate\n",
    "            forward_proj = np.zeros(n_subset_lors, dtype=np.float32)\n",
    "            \n",
    "            for i, lor_idx in enumerate(indices):\n",
    "                p1 = xyz1[lor_idx].astype(np.float64)\n",
    "                p2 = xyz2[lor_idx].astype(np.float64)\n",
    "                forward_proj[i] = forward_project_lor(\n",
    "                    p1, p2, image, mu_map, n_voxels, voxel_size, half_fov\n",
    "                )\n",
    "            \n",
    "            # Compute ratio (measured / estimated)\n",
    "            # Measured = 1 for each detected LOR\n",
    "            ratios = np.ones(n_subset_lors, dtype=np.float32)\n",
    "            mask = forward_proj > 1e-10\n",
    "            ratios[mask] = 1.0 / forward_proj[mask]\n",
    "            \n",
    "            # Backproject ratios\n",
    "            backproj = np.zeros((n_voxels, n_voxels, n_voxels), dtype=np.float32)\n",
    "            \n",
    "            for i, lor_idx in enumerate(indices):\n",
    "                p1 = xyz1[lor_idx].astype(np.float64)\n",
    "                p2 = xyz2[lor_idx].astype(np.float64)\n",
    "                siddon_backproject_lor(\n",
    "                    p1, p2, n_voxels, voxel_size, half_fov, ratios[i], backproj\n",
    "                )\n",
    "            \n",
    "            # Update image: image *= backproj / sensitivity\n",
    "            # Scale by number of subsets to account for subset sampling\n",
    "            update = backproj / sensitivity\n",
    "            image *= update\n",
    "            \n",
    "            # Enforce positivity\n",
    "            image = np.maximum(image, 1e-10)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Iteration {iteration+1}/{n_iterations}, \"\n",
    "                      f\"Subset {subset_idx+1}/{n_subsets}, \"\n",
    "                      f\"Image range: [{image.min():.4f}, {image.max():.4f}]\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nReconstruction complete!\")\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RUN RECONSTRUCTION\n",
    "# =============================================================================\n",
    "# Assuming 'data' contains xyz1 and xyz2 from your LOR extraction\n",
    "osem_image = osem_reconstruct(\n",
    "    lor_data['xyz1'], lor_data['xyz2'],\n",
    "    mu_map=attenuation_map,\n",
    "    voxel_size=reconstruction.voxel_size,\n",
    "    fov=reconstruction.fov,\n",
    "    n_iterations=1,\n",
    "    n_subsets=4,\n",
    ")\n",
    "\n",
    "# np.save(\"osem_reconstruction.npy\", osem_image) # Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb8e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Animate through slices\n",
    "fig = px.imshow(\n",
    "    osem_image,\n",
    "    animation_frame=2,\n",
    "    zmax=osem_image.max(),\n",
    "    color_continuous_scale='Hot',\n",
    ").show()\n",
    "\n",
    "fig = px.imshow(\n",
    "    ground_truth_activity,\n",
    "    animation_frame=2,\n",
    "    zmax=ground_truth_activity.max(),\n",
    "    color_continuous_scale='Hot',\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opengate_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
